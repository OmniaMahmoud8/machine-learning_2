# -*- coding: utf-8 -*-
"""notebook37aaf711ea.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lmM9vCu3AT8urofApJ6Z77V2gDx-Cp7n
"""

!pip install Arabic-Stopwords

!pip install emoji

!pip install PyArabic

!pip install openpyxl

!pip install Tashaphyne

!pip install nltk.download('stopwords')

import pandas as pd

df=pd.read_csv('/content/Final_Data.csv')

df.head()

df.rename(columns ={'rating':'label'},inplace =True)

df.shape

df.info()

df.label.value_counts()

# Commented out IPython magic to ensure Python compatibility.
# Add environment Packages paths to conda
import os, sys, warnings
import pandas as pd
import numpy as np
warnings.simplefilter("ignore")

# Text preprocessing packages
import nltk # Text libarary
# nltk.download('stopwords')
import string # Removing special characters {#, @, ...}
import re # Regex Package
import regex
import emoji
# Corpora is a group presenting multiple collections of text documents. A single collection is called corpus.
from nltk.corpus import stopwords # Stopwords
import arabicstopwords.arabicstopwords as stp #more range of arabic stop words
from nltk.stem.isri import ISRIStemmer
import pyarabic.araby as araby
from tashaphyne.stemming import ArabicLightStemmer

from nltk.stem import SnowballStemmer, WordNetLemmatizer # Stemmer & Lemmatizer
#from gensim.utils import simple_preprocess  # Text ==> List of Tokens

# Text Embedding
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Visualization Packages
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(font_scale=1.3)
# %matplotlib inline

plt.figure(figsize=(8,4))
sns.countplot(x='label',data=df)

df.isnull().sum()

df[df['review_description'].isnull() == True]

df.duplicated().sum()

df[df['review_description'].duplicated() == True]

df=df.dropna()

df=df.drop_duplicates()

df.duplicated().sum()

df.isnull().sum()

import nltk
nltk.download('stopwords')

arabic_stopwords = stopwords.words("arabic")
arabic_stopwords

len(arabic_stopwords)

egyptian_stopwords_base = [
    'ÙŠØ¹Ù†ÙŠ', 'ÙƒØ¯Ù‡', 'Ø£ÙˆÙŠ', 'Ø¨ØªØ§Ø¹', 'Ø¨ØªØ§Ø¹Ø©', 'Ø¥Ø²Ø§ÙŠ', 'Ø¥ÙŠÙ‡', 'Ø¨Ø³', 'Ø²ÙŠ', 'Ø¯ÙŠ',
    'Ø¯Ù‡', 'Ø¯Ø§', 'ÙÙŠÙ†', 'Ù„ÙŠÙ‡', 'Ø¥Ù…ØªÙ‰', 'Ù…ÙŠÙ†',
    'ÙƒØ§Ù…', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ø¬ÙˆØ©', 'Ø¨Ø±Ø§', 'ÙÙˆÙ‚', 'ØªØ­Øª', 'Ø¬ÙˆÙ‡', 'Ù‚Ø¯Ø§Ù…', 'ÙˆØ±Ø§',
    'Ø¬Ù†Ø¨', 'Ø¹Ù„Ø´Ø§Ù†', 'Ø£Ù‡Ùˆ', 'Ø¨Ù‚Ù‰', 'Ù„Ø³Ù‡', 'ÙƒØ¯Ø©', 'ÙŠØ¹Ù†ÙŠÙ‡', 'Ø¥Ø­Ù†Ø§', 'Ø¥Ù†Øª', 'Ø¥Ù†ØªÙˆØ§',
    'Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ø¯ÙˆÙ„', 'ÙƒÙ…Ø§Ù†', 'Ø¨Ø±Ø¶Ùˆ', 'Ø£ÙƒÙŠØ¯', 'Ø·Ø¨', 'Ø¨Ø¹Ø¯ÙŠÙ†', 'Ø£ÙŠÙˆÙ‡', 'Ù„Ø§'
    , 'ÙƒÙ„Ù‡', 'ÙƒÙ„Ù‡Ø§', 'Ø£ÙŠ', 'Ø¨ØªÙˆØ¹', 'Ø­Ø§Ù„ÙŠÙ‹Ø§', 'ØªÙˆ', 'Ø¯Ù„ÙˆÙ‚ØªÙŠ', 'Ø¨ÙƒØ±Ø©', 'Ø¥Ù…Ø¨Ø§Ø±Ø­',
    'ÙƒÙ„', 'ÙƒÙ„Ù†Ø§', 'ÙƒÙ„Ù‡Ù…', 'Ø¨Ø³Ø±Ø¹Ø©', 'Ø´ÙˆÙŠØ©', 'Ø´ÙˆÙŠØªÙŠÙ†', 'Ø®Ø§Ù„Øµ', 'Ùˆ', 'ÙŠØ§', 'Ù„Ùˆ',
    'Ù„Ù…Ø§', 'Ù„Ù…Ù†', 'Ù„ØºØ§ÙŠØ©', 'Ù…Ù†ÙŠÙ†', 'Ù„Ø­ØªÙ‰', 'Ø¹Ù„ÙŠ', 'Ø¹Ù†', 'Ù‚Ø¨Ù„', 'Ø¨Ø¹Ø¯', 'ØªØ­ØªÙŠ',
    'ÙÙˆÙ‚ÙŠ', 'Ø¬Ù†Ø¨ÙŠ', 'ÙˆØ±Ø§ÙŠØ§', 'Ù‚Ø¯Ø§Ù…ÙŠ', 'Ø¬ÙˆØ§ÙŠØ§', 'Ø¨Ø±Ù‡', 'Ø£Ù‡', 'Ø§Ù‡Ø§', 'Ø¥ÙŠ', 'Ø¥ÙˆØ§',
    'Ø¨Ù‚Ø§', 'Ø¨ØªÙ‚ÙˆÙ„', 'Ø¨ØªÙ‚ÙˆÙ„ÙˆØ§', 'ÙŠØ¹Ù†ÙˆØ§', 'ÙƒØ¯ÙŠ', 'Ø¥Ø²ÙŠÙƒ', 'Ø¥Ø²ÙŠ', 'Ø¥Ø²ÙŠÙƒÙˆ', 'Ø¥Ø²ÙŠÙ‡',
    'Ø¥Ø²ÙŠÙ‡Ù…', 'Ø¥Ø²ÙŠÙ†Ø§', 'Ø¥Ø²ÙŠÙƒÙ…', 'Ø·Ø¨Ø¹Ù‹Ø§', 'Ø£ØµÙ„Ù‹Ø§', 'ÙÙŠÙ†Ùƒ', 'ÙÙŠÙ†ÙƒÙ…', 'ÙÙŠÙ†Ù‡Ù…', 'ÙÙŠÙ†Ù‡',
    'ÙÙŠÙ†Ù‡Ø§', 'ÙÙŠÙ†Ùˆ', 'Ø¥ÙŠÙ‡Ø¯Ù‡', 'Ø¥ÙŠØ¯Ø§', 'Ø¥ÙŠØ¯ÙŠ', 'Ø¥ÙŠØ¯Ùƒ', 'Ø¥ÙŠØ¯Ù‡', 'Ø¥ÙŠØ¯Ù‡Ø§', 'Ø¥ÙŠØ¯Ù‡Ùˆ',
    'Ø¥ÙŠØ¯Ù‡ÙˆÙ…', 'ÙƒØ¯Ø§Ùƒ', 'ÙƒØ¯Ø§Ù‡', 'ÙƒØ¯Ø§Ù‡Ø§', 'ÙƒØ¯Ø§Ù‡Ùˆ', 'ÙƒØ¯Ø§Ù‡Ù…', 'Ø§Ù†Ø§', 'ÙˆØ§Ù„Ù„Ù‡', 'Ø´ÙŠ',
    'ÙƒØ§Ù†', 'Ø¨Ø¹Ø¶', 'ØªÙ…', 'ÙÙ‰'
]

egyptian_stopwords_extra = [
    'Ø¨ØªØ§Ø¹Ù‡Ù…', 'Ø¨ØªØ§Ø¹Ù†Ø§', 'Ø¨ØªØ§Ø¹ÙƒÙ…', 'Ø¨ØªØ§Ø¹ÙŠ', 'Ø¨ØªØ§Ø¹Ù‡', 'Ø¨ØªØ§Ø¹Ù‡Ø§', 'Ø¨ØªØ§Ø¹Ùˆ', 'Ø²ÙŠÙƒ',
    'Ø²ÙŠÙ‡', 'Ø²ÙŠÙ‡Ø§', 'Ø²ÙŠÙ‡Ù…', 'Ø²ÙŠÙ†Ø§', 'Ø²ÙŠÙƒÙ…', 'Ø²ÙŠÙˆ', 'ÙƒØ¯ÙŠÙ‡', 'ÙƒØ¯ÙˆÙ‡', 'ÙƒØ¯Ø§Ù‡Ø§', 'ÙƒØ¯Ø§Ù‡Ù…',
    'ÙƒØ¯Ø§Ù‡Ùˆ', 'Ø¥Ø²ÙŠÙƒÙŠ', 'Ø¥Ø²ÙŠÙƒÙˆØ§', 'Ø¥Ø²ÙŠÙ‡Ù…', 'Ø¥Ø²ÙŠÙ†Ø§', 'Ø¥Ø²ÙŠÙƒÙˆÙ…', 'ÙÙŠÙ†ÙŠ', 'ÙÙŠÙƒ', 'ÙÙŠÙ‡Ùˆ',
    'ÙÙŠÙ‡Ø§', 'ÙÙŠÙ‡Ù…', 'ÙÙŠÙƒÙ…', 'ÙÙŠÙ†Ø§', 'Ø¥ÙŠØ¯Ù‡ÙŠ', 'Ø¥ÙŠØ¯Ù‡ÙˆÙ…', 'Ø¥ÙŠØ¯ÙŠÙ†Ø§', 'Ø¥ÙŠØ¯ÙŠÙƒ', 'Ø¥ÙŠØ¯ÙŠÙƒÙ…',
    'Ø¥ÙŠØ¯ÙŠÙ‡Ù…', 'Ø¥ÙŠØ¯ÙŠÙ‡', 'Ø¨ØªÙ‚ÙˆÙ„ÙŠ', 'Ø¨ØªÙ‚ÙˆÙ„Ùˆ', 'Ø¨ØªÙ‚ÙˆÙ„Ù‡Ù…', 'Ø¨ØªÙ‚ÙˆÙ„Ù†Ø§', 'Ø¨ØªÙ‚ÙˆÙ„Ùƒ', 'Ø¨ØªÙ‚ÙˆÙ„ÙƒÙ…',
    'ÙŠØ¹Ù†ÙŠÙƒÙŠ', 'ÙŠØ¹Ù†ÙŠÙ‡Ù…', 'ÙŠØ¹Ù†ÙŠÙ†ÙŠ', 'ÙŠØ¹Ù†ÙŠÙƒÙˆÙ…', 'ÙŠØ¹Ù†ÙŠÙ‡Ø§', 'ÙŠØ¹Ù†ÙŠÙ‡Ùˆ', 'Ø£Ù‡ÙˆÙ‡', 'Ø£Ù‡ÙŠ',
    'Ø£Ù‡ÙŠØ§', 'Ø£Ù‡ÙŠÙ‡', 'Ø£Ù‡ÙŠÙ‡Ù…', 'Ø£Ù‡ÙŠÙ‡Ø§', 'Ø£Ù‡ÙŠÙˆ', 'Ø¨Ù‚ÙŠÙ†Ø§', 'Ø¨Ù‚ÙŠØªÙˆØ§', 'Ø¨Ù‚ÙˆØ§', 'Ø¨Ù‚ÙŠØª',
    'Ø¨Ù‚ÙŠØªÙŠ', 'Ø¨Ù‚ÙŠ', 'Ù„Ø³Ù†Ø§', 'Ù„Ø³Ù†ÙŠ', 'Ù„Ø³Ùƒ',
    'Ù„Ø³ÙƒÙ…', 'Ù„Ø³Ù‡Ø§', 'Ù„Ø³ÙˆØ§',  'Ø£ÙˆÙƒÙŠÙƒÙ…', 'ÙÙŠÙ†ÙƒÙŠ', 'ÙÙŠÙ†ÙƒÙˆ', 'ÙÙŠÙ†Ù‡Ù…', 'ÙÙŠÙ†Ù‡Ùˆ',
    'ÙÙŠÙ†Ù‡Ø§', 'ÙÙŠÙ†ÙŠ', 'ÙÙŠÙƒÙŠ', 'ÙÙŠÙƒÙˆ', 'Ø¥Ø­Ù†Ø§Ùƒ', 'Ø¥Ø­Ù†Ø§Ù‡', 'Ø¥Ø­Ù†Ø§Ù‡Ø§', 'Ø¥Ø­Ù†Ø§Ù‡Ù…', 'Ø¥Ø­Ù†Ø§Ù†Ø§',
    'Ø¥Ø­Ù†Ø§ÙƒÙ…', 'Ø¥Ù†ØªÙŠ', 'Ø¥Ù†ØªÙˆ', 'Ø¥Ù†ØªÙƒ', 'Ø¥Ù†ØªÙ‡Ø§', 'Ø¥Ù†ØªÙ‡Ù…', 'Ø¥Ù†ØªÙ†Ø§', 'Ø¥Ù†ØªÙƒÙ…', 'Ù‡ÙˆÙŠ',
    'Ù‡ÙˆÙ‡', 'Ù‡ÙˆÙ‡Ø§', 'Ù‡ÙˆÙ‡Ù…', 'Ù‡ÙˆÙ†Ø§', 'Ù‡ÙˆÙƒÙ…', 'Ù‡ÙŠÙŠ', 'Ù‡ÙŠÙ‡', 'Ù‡ÙŠÙ‡Ø§', 'Ù‡ÙŠÙ‡Ù…', 'Ù‡ÙŠÙ†Ø§',
    'Ù‡ÙŠÙƒÙ…', 'Ø¯ÙˆÙ„ÙŠ', 'Ø¯ÙˆÙ„Ù‡', 'Ø¯ÙˆÙ„Ù‡Ø§', 'Ø¯ÙˆÙ„Ù‡Ù…', 'Ø¯ÙˆÙ„Ù†Ø§', 'Ø¯ÙˆÙ„ÙƒÙ…', 'Ø¯ÙŠÙŠ', 'Ø¯ÙŠÙ‡',
    'Ø¯ÙŠÙ‡Ø§', 'Ø¯ÙŠÙ‡Ù…', 'Ø¯ÙŠÙ†Ø§', 'Ø¯ÙŠÙƒÙ…', 'ÙƒÙ…Ø§Ù†ÙŠ', 'ÙƒÙ…Ø§Ù†Ù‡', 'ÙƒÙ…Ø§Ù†Ù‡Ø§', 'ÙƒÙ…Ø§Ù†Ù‡Ù…', 'ÙƒÙ…Ø§Ù†Ù†Ø§',
    'ÙƒÙ…Ø§Ù†ÙƒÙ…', 'Ø¨Ø±Ø¶ÙˆÙŠ', 'Ø¨Ø±Ø¶ÙˆÙ‡', 'Ø¨Ø±Ø¶ÙˆÙ‡Ø§', 'Ø¨Ø±Ø¶ÙˆÙ‡Ù…', 'Ø¨Ø±Ø¶ÙˆÙ†Ø§', 'Ø¨Ø±Ø¶ÙˆÙƒÙ…', 'Ø£ÙƒÙŠØ¯ÙŠ',
    'Ø£ÙƒÙŠØ¯Ù‡', 'Ø£ÙƒÙŠØ¯Ù‡Ø§', 'Ø£ÙƒÙŠØ¯Ù‡Ù…', 'Ø£ÙƒÙŠØ¯Ù†Ø§', 'Ø£ÙƒÙŠØ¯ÙƒÙ…', 'Ø·Ø¨ÙŠ', 'Ø·Ø¨Ù‡', 'Ø·Ø¨Ù‡Ø§', 'Ø·Ø¨Ù‡Ù…',
    'Ø·Ø¨Ù†Ø§', 'Ø·Ø¨ÙƒÙ…', 'Ø¨Ø¹Ø¯ÙŠÙ†ÙŠ', 'Ø¨Ø¹Ø¯ÙŠÙ†Ù‡', 'Ø¨Ø¹Ø¯ÙŠÙ†Ù‡Ø§', 'Ø¨Ø¹Ø¯ÙŠÙ†Ù‡Ù…', 'Ø¨Ø¹Ø¯ÙŠÙ†Ù†Ø§', 'Ø¨Ø¹Ø¯ÙŠÙ†ÙƒÙ…',
    'Ø£ÙŠÙˆÙ‡ÙŠ', 'Ø£ÙŠÙˆÙ‡Ù‡', 'Ø£ÙŠÙˆÙ‡Ù‡Ø§', 'Ø£ÙŠÙˆÙ‡Ù‡Ù…', 'Ø£ÙŠÙˆÙ‡Ù†Ø§', 'Ø£ÙŠÙˆÙ‡ÙƒÙ…', 'Ù„Ø§Ù‡', 'Ù„Ø§Ù‡Ø§', 'Ù„Ø§Ù‡Ù…',
    'Ù„Ø§Ù†Ø§', 'Ù„Ø§ÙƒÙ…', 'Ø¹Ø§Ø¯ÙŠÙ‡Ø§', 'Ø¹Ø§Ø¯ÙŠÙ‡Ù…', 'Ø¹Ø§Ø¯ÙŠÙ†Ø§', 'Ø¹Ø§Ø¯ÙŠÙƒÙ…', 'ÙƒÙ„Ù‡ÙŠ',
    'ÙƒÙ„Ù‡Ù‡', 'ÙƒÙ„Ù‡Ù‡Ø§', 'ÙƒÙ„Ù‡Ù‡Ù…', 'ÙƒÙ„Ù†ÙŠ', 'ÙƒÙ„Ù†Ù‡', 'ÙƒÙ„Ù†Ù‡Ø§', 'ÙƒÙ„Ù†Ù‡Ù…', 'ÙƒÙ„Ù†Ù†Ø§', 'ÙƒÙ„Ù†ÙƒÙ…',
    'Ø¨Ø³Ø±Ø¹ØªÙ‡', 'Ø¨Ø³Ø±Ø¹ØªÙ‡Ø§', 'Ø¨Ø³Ø±Ø¹ØªÙ‡Ù…', 'Ø¨Ø³Ø±Ø¹ØªÙ†Ø§', 'Ø¨Ø³Ø±Ø¹ØªÙƒÙ…', 'Ø´ÙˆÙŠÙ‡', 'Ø´ÙˆÙŠØ§Øª', 'Ø´ÙˆÙŠÙ†Ø§',
    'Ø´ÙˆÙŠÙƒ', 'Ø´ÙˆÙŠÙƒÙ…', 'Ø´ÙˆÙŠÙ‡Ù…', 'Ø´ÙˆÙŠÙ‡Ø§', 'Ø´ÙˆÙŠÙ‡Ùˆ', 'Ø®Ø§Ù„ØµÙŠ', 'Ø®Ø§Ù„ØµÙ‡', 'Ø®Ø§Ù„ØµÙ‡Ø§', 'Ø®Ø§Ù„ØµÙ‡Ù…',
    'Ø®Ø§Ù„ØµÙ†Ø§', 'Ø®Ø§Ù„ØµÙƒÙ…', 'ÙˆÙˆ', 'ÙˆÙŠØ§', 'ÙˆÙ„Ùˆ', 'ÙˆÙ„Ù…Ø§', 'ÙˆÙ„Ù…Ù†', 'ÙˆÙ„ØºØ§ÙŠØ©', 'ÙˆÙ…Ù†ÙŠÙ†', 'ÙˆÙ„Ø­ØªÙ‰',
    'ÙˆØ¹Ù„ÙŠ', 'ÙˆØ¹Ù†', 'ÙˆÙ‚Ø¨Ù„', 'ÙˆØ¨Ø¹Ø¯', 'ÙˆØªØ­Øª', 'ÙˆÙÙˆÙ‚', 'ÙˆØ¬Ù†Ø¨', 'ÙˆÙˆØ±Ø§', 'ÙˆÙ‚Ø¯Ø§Ù…', 'ÙˆØ¬ÙˆØ©',
    'ÙˆØ¨Ø±Ù‡', 'ÙˆØ£Ù‡', 'ÙˆØ§Ù‡Ø§', 'ÙˆØ¥ÙŠ', 'ÙˆØ¥ÙˆØ§', 'ÙˆØ¨Ù‚Ø§', 'ÙˆØ¨ØªÙ‚ÙˆÙ„', 'ÙˆØ¨ØªÙ‚ÙˆÙ„ÙˆØ§', 'ÙˆÙŠØ¹Ù†ÙŠ', 'ÙˆÙƒØ¯Ù‡',
    'ÙˆØ¥Ø²Ø§ÙŠ', 'ÙˆØ¥ÙŠÙ‡', 'ÙˆØ¨Ø³', 'ÙˆØ²ÙŠ', 'ÙˆØ¯ÙŠ', 'ÙˆØ¯Ù‡', 'ÙˆØ¯Ø§', 'ÙˆÙ…Ø§Ø´ÙŠ', 'ÙˆØ·ÙŠØ¨', 'ÙˆØªÙ…Ø§Ù…',
    'ÙˆØ£ÙˆÙƒÙŠ', 'ÙˆÙÙŠÙ†', 'ÙˆÙ„ÙŠÙ‡', 'ÙˆØ¥Ù…ØªÙ‰', 'ÙˆÙ…ÙŠÙ†', 'ÙˆÙƒØ§Ù…', 'ÙˆÙ‡Ù†Ø§', 'ÙˆÙ‡Ù†Ø§Ùƒ', 'ÙˆØ¬ÙˆØ©', 'ÙˆØ¨Ø±Ø§',
    'ÙˆÙÙˆÙ‚', 'ÙˆØªØ­Øª', 'ÙˆØ¬ÙˆÙ‡', 'ÙˆÙ‚Ø¯Ø§Ù…', 'ÙˆÙˆØ±Ø§', 'ÙˆØ¬Ù†Ø¨', 'ÙˆØ¹Ù„Ø´Ø§Ù†', 'ÙˆØ£Ù‡Ùˆ', 'ÙˆØ¨Ù‚Ù‰', 'ÙˆÙ„Ø³Ù‡',
    'ÙˆÙƒØ¯Ø©', 'ÙˆÙŠØ¹Ù†ÙŠÙ‡', 'ÙˆØ¥Ø­Ù†Ø§', 'ÙˆØ¥Ù†Øª', 'ÙˆØ¥Ù†ØªÙˆØ§', 'ÙˆÙ‡Ùˆ', 'ÙˆÙ‡ÙŠ', 'ÙˆØ¯ÙˆÙ„', 'ÙˆÙƒÙ…Ø§Ù†', 'ÙˆØ¨Ø±Ø¶Ùˆ',
    'ÙˆØ£ÙƒÙŠØ¯', 'ÙˆØ·Ø¨', 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†', 'ÙˆØ£ÙŠÙˆÙ‡', 'ÙˆÙ„Ø§', 'ÙˆØ¹Ø§Ø¯ÙŠ', 'ÙˆÙƒÙ„Ù‡', 'ÙˆÙƒÙ„Ù‡Ø§', 'ÙˆØ£ÙŠ', 'ÙˆØ¨ØªÙˆØ¹',
    'ÙˆØ­Ø§Ù„ÙŠÙ‹Ø§', 'ÙˆØªÙˆ', 'ÙˆØ¯Ù„ÙˆÙ‚ØªÙŠ', 'ÙˆØ¨ÙƒØ±Ø©', 'ÙˆØ¥Ù…Ø¨Ø§Ø±Ø­', 'ÙˆÙƒÙ„', 'ÙˆÙƒÙ„Ù†Ø§', 'ÙˆÙƒÙ„Ù‡Ù…', 'ÙˆØ¨Ø³Ø±Ø¹Ø©',
    'Ø¥Ù„Ù„ÙŠ', 'Ø§Ù„Ù„ÙŠ', 'Ø§Ù„ÙŠ', 'Ø§Ù„Ù„', 'Ø§Ù„', 'Ø§Ù„Ùˆ', 'Ø§Ù„ÙŠ', 'Ø§Ù„Ù„ÙŠÙŠ', 'Ø§Ù„Ù„ÙŠÙ‡', 'Ø§Ù„Ù„ÙŠÙ‡Ø§', 'Ø§Ù„Ù„ÙŠÙ‡Ù…',
    'Ø§Ù„Ù„ÙŠÙ†Ø§', 'Ø§Ù„Ù„ÙŠÙƒÙ…', 'Ø¹Ù„ÙŠÙ‡', 'Ø¹Ù„ÙŠÙ‡Ø§', 'Ø¹Ù„ÙŠÙ‡Ù…', 'Ø¹Ù„ÙŠÙ†Ø§', 'Ø¹Ù„ÙŠÙƒÙ…', 'Ø¹Ù†Ù‡', 'Ø¹Ù†Ù‡Ø§', 'Ø¹Ù†Ù‡Ù…',
    'Ø¹Ù†Ù†Ø§', 'Ø¹Ù†ÙƒÙ…', 'Ù‚Ø¨Ù„Ù‡', 'Ù‚Ø¨Ù„Ù‡Ø§', 'Ù‚Ø¨Ù„Ù‡Ù…', 'Ù‚Ø¨Ù„Ù†Ø§', 'Ù‚Ø¨Ù„ÙƒÙ…', 'Ø¨Ø¹Ø¯Ù‡', 'Ø¨Ø¹Ø¯Ù‡Ø§', 'Ø¨Ø¹Ø¯Ù‡Ù…',
    'Ø¨Ø¹Ø¯Ù†Ø§', 'Ø¨Ø¹Ø¯ÙƒÙ…', 'ØªØ­ØªÙ‡', 'ØªØ­ØªÙ‡Ø§', 'ØªØ­ØªÙ‡Ù…', 'ØªØ­ØªÙ†Ø§', 'ØªØ­ØªÙƒÙ…', 'ÙÙˆÙ‚Ù‡', 'ÙÙˆÙ‚Ù‡Ø§', 'ÙÙˆÙ‚Ù‡Ù…',
    'ÙÙˆÙ‚Ù†Ø§', 'ÙÙˆÙ‚ÙƒÙ…', 'Ø¬Ù†Ø¨Ù‡', 'Ø¬Ù†Ø¨Ù‡Ø§', 'Ø¬Ù†Ø¨Ù‡Ù…', 'Ø¬Ù†Ø¨Ù†Ø§', 'Ø¬Ù†Ø¨ÙƒÙ…', 'ÙˆØ±Ø§Ù‡', 'ÙˆØ±Ø§Ù‡Ø§', 'ÙˆØ±Ø§Ù‡Ù…',
    'ÙˆØ±Ø§Ù†Ø§', 'ÙˆØ±Ø§ÙƒÙ…', 'Ù‚Ø¯Ø§Ù…Ù‡', 'Ù‚Ø¯Ø§Ù…Ù‡Ø§', 'Ù‚Ø¯Ø§Ù…Ù‡Ù…', 'Ù‚Ø¯Ø§Ù…Ù†Ø§', 'Ù‚Ø¯Ø§Ù…ÙƒÙ…', 'Ø¬ÙˆØ§Ù‡', 'Ø¬ÙˆØ§Ù‡Ø§',
    'Ø¬ÙˆØ§Ù‡Ù…', 'Ø¬ÙˆØ§Ù†Ø§', 'Ø¬ÙˆØ§ÙƒÙ…', 'Ø¨Ø±Ø§Ù‡', 'Ø¨Ø±Ø§Ù‡Ø§', 'Ø¨Ø±Ø§Ù‡Ù…', 'Ø¨Ø±Ø§Ù†Ø§', 'Ø¨Ø±Ø§ÙƒÙ…', 'Ø£Ù‡ÙŠ', 'Ø£Ù‡ÙŠÙ‡',
    'Ø£Ù‡ÙŠÙ‡Ø§', 'Ø£Ù‡ÙŠÙ‡Ù…', 'Ø£Ù‡ÙŠÙ†Ø§', 'Ø£Ù‡ÙŠÙƒÙ…', 'Ø¨Ù‚Ø§Ù‡', 'Ø¨Ù‚Ø§Ù‡Ø§', 'Ø¨Ù‚Ø§Ù‡Ù…', 'Ø¨Ù‚Ø§Ù†Ø§', 'Ø¨Ù‚Ø§ÙƒÙ…', 'Ù„Ø³Ù‡ÙŠ',
    'Ù„Ø³Ù‡Ù‡', 'Ù„Ø³Ù‡Ù‡Ø§', 'Ù„Ø³Ù‡Ù‡Ù…', 'Ù„Ø³Ù†Ø§', 'Ù„Ø³Ù‡ÙƒÙ…'
]

from collections import Counter

df['review_description'] = df['review_description'].astype(str)
all_text = ' '.join(df['review_description'])
words = all_text.split()
word_freq = Counter(words)

# Ø·Ø¨Ø§Ø¹Ø© Ø£ÙƒØ«Ø± 20 ÙƒÙ„Ù…Ø© Ø´ÙŠÙˆØ¹Ù‹Ø§
print("the most 100 word frequent", word_freq.most_common(100))

df['stop_words'] = df['review_description'].apply(lambda x: len(set(x.split()) & set(arabic_stopwords)))
df.stop_words.value_counts()

df.head()

sns.histplot(df['stop_words'], bins=20, color='purple', kde=False)
plt.title('Distribution of Stop Words in Texts', fontsize=14)
plt.xlabel('Number of Stop Words', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.show()

def emoji_counter(sentence):
    return emoji.emoji_count(sentence)

df['emoji_count'] = df['review_description'].apply(lambda x: emoji_counter(x))
df.emoji_count.value_counts()[:10]

df.head()

emojis_data = df[df['emoji_count'] > 0]

df.info()

emojis_data

df = df.drop(df[df['review_description'] == "Ø¬Ø­Ù‡Ù†Ù†Ø§ØºÙ†Ù…Ø¬Ø¸ Ø¬Ø­Ø®ØªØ§Ù‰Ø©Ø²ÙˆÙ†Ù‡ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹"].index)

emojis_data

def extract_emoji(text):
    emoji_list = []
    data = regex.findall(r'\X', text)
    for word in data:
        if any(emoji.distinct_emoji_list(char) for char in word):
            emoji_list.append(word)

    return emoji_list

emojis_data['emoji'] = emojis_data['review_description'].apply(lambda x: extract_emoji(x))
emojis_data['emoji'] = emojis_data['emoji'].apply(lambda x: ' '.join([word for word in x ]))

emojis_data

pd.Series(' '.join(emojis_data['emoji']).split()).value_counts()[:10]

emojis = {
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "ğŸ˜‚":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ’”":"Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†",
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "â¤":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø¨",
    "ğŸ˜­":"ÙŠØ¨ÙƒÙŠ",
    "ğŸ˜¢":"Ø­Ø²Ù†",
    "ğŸ˜”":"Ø­Ø²Ù†",
    "â™¥":"Ø­Ø¨",
    "ğŸ’œ":"Ø­Ø¨",
    "ğŸ˜…":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ™":"Ø­Ø²ÙŠÙ†",
    "ğŸ’•":"Ø­Ø¨",
    "ğŸ’™":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜Š":"Ø³Ø¹Ø§Ø¯Ø©",
    "ğŸ‘":"ÙŠØµÙÙ‚",
    "ğŸ‘Œ":"Ø§Ø­Ø³Ù†Øª",
    "ğŸ˜´":"ÙŠÙ†Ø§Ù…",
    "ğŸ˜€":"ÙŠØ¶Ø­Ùƒ",
 "ğŸ˜Œ":"Ø­Ø²ÙŠÙ†",
    "ğŸŒ¹":"ÙˆØ±Ø¯Ø©",
    "ğŸ™ˆ":"Ø­Ø¨",
    "ğŸ˜„":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜":"Ù…Ø­Ø§ÙŠØ¯",
    "âœŒ":"Ù…Ù†ØªØµØ±",
    "âœ¨":"Ù†Ø¬Ù…Ù‡",
    "ğŸ¤”":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ˜’":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ™„":"Ù…Ù„Ù„",
    "ğŸ˜•":"Ø¹ØµØ¨ÙŠØ©",
    "ğŸ˜ƒ":"ÙŠØ¶Ø­Ùƒ",
    "ğŸŒ¸":"ÙˆØ±Ø¯Ø©",
    "ğŸ˜“":"Ø­Ø²Ù†",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ’—":"Ø­Ø¨",
    "ğŸ˜‘":"Ù…Ù†Ø²Ø¹Ø¬",
    "ğŸ’­":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"Ø«Ù‚Ø©",
    "ğŸ’›":"Ø­Ø¨",
    "ğŸ˜©":"Ø­Ø²ÙŠÙ†",
    "ğŸ’ª":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ‘":"Ù…ÙˆØ§ÙÙ‚",
    "ğŸ™ğŸ»":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ˜³":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ‘ğŸ¼":"ØªØµÙÙŠÙ‚",
    "ğŸ¶":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸŒš":"ØµÙ…Øª",
    "ğŸ’š":"Ø­Ø¨",
    "ğŸ™":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’˜":"Ø­Ø¨",
    "ğŸƒ":"Ø³Ù„Ø§Ù…",
    "â˜º":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ¸":"Ø¶ÙØ¯Ø¹",
    "ğŸ˜¶":"Ù…ØµØ¯ÙˆÙ…",
    "âœŒï¸":"Ù…Ø±Ø­",
    "âœ‹ğŸ»":"ØªÙˆÙ‚Ù",
    "ğŸ˜‰":"ØºÙ…Ø²Ø©",
    "ğŸŒ·":"Ø­Ø¨",
    "ğŸ™ƒ":"Ù…Ø¨ØªØ³Ù…",
    "ğŸ˜«":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜¨":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ¼ ":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸ":"Ù…Ø±Ø­",
    "ğŸ‚":"Ù…Ø±Ø­",
    "ğŸ’Ÿ":"Ø­Ø¨",
    "ğŸ˜ª":"Ø­Ø²Ù†",
    "ğŸ˜†":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜£":"Ø§Ø³ØªÙŠØ§Ø¡",
    "â˜ºï¸":"Ø­Ø¨",
    "ğŸ˜±":"ÙƒØ§Ø±Ø«Ø©",
    "ğŸ˜":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜–":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸƒğŸ¼":"ÙŠØ¬Ø±ÙŠ",
    "ğŸ˜¡":"ØºØ¶Ø¨",
    "ğŸš¶":"ÙŠØ³ÙŠØ±",
    "ğŸ¤•":"Ù…Ø±Ø¶",
    "â€¼ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ•Š":"Ø·Ø§Ø¦Ø±",
    "ğŸ‘ŒğŸ»":"Ø§Ø­Ø³Ù†Øª",
    "â£":"Ø­Ø¨",
    "ğŸ™Š":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ’ƒ":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ’ƒğŸ¼":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ˜œ":"Ù…Ø±Ø­",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜Ÿ":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸ’–":"Ø­Ø¨",
    "ğŸ˜¥":"Ø­Ø²Ù†",
    "ğŸ»":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "âœ’":"ÙŠÙƒØªØ¨",
    "ğŸš¶ğŸ»":"ÙŠØ³ÙŠØ±",
    "ğŸ’":"Ø§Ù„Ù…Ø§Ø¸",
    "ğŸ˜·":"ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶",
    "â˜":"ÙˆØ§Ø­Ø¯",
    "ğŸš¬":"ØªØ¯Ø®ÙŠÙ†",
    "ğŸ’" : "ÙˆØ±Ø¯",
    "ğŸŒ" : "Ø´Ù…Ø³",
    "ğŸ‘†" : "Ø§Ù„Ø§ÙˆÙ„",
    "âš ï¸" :"ØªØ­Ø°ÙŠØ±",
    "ğŸ¤—" : "Ø§Ø­ØªÙˆØ§Ø¡",
    "âœ–ï¸": "ØºÙ„Ø·",
    "ğŸ“"  : "Ù…ÙƒØ§Ù†",
    "ğŸ‘¸" : "Ù…Ù„ÙƒÙ‡",
    "ğŸ‘‘" : "ØªØ§Ø¬",
    "âœ”ï¸" : "ØµØ­",
    "ğŸ’Œ": "Ù‚Ù„Ø¨",
     "ğŸ˜²" : "Ù…Ù†Ø¯Ù‡Ø´",
    "ğŸ’¦": "Ù…Ø§Ø¡",
    "ğŸš«" : "Ø®Ø·Ø§",
    "ğŸ‘ğŸ»" : "Ø¨Ø±Ø§ÙÙˆ",
    "ğŸŠ" :"ÙŠØ³Ø¨Ø­",
    "ğŸ‘ğŸ»": "ØªÙ…Ø§Ù…",
    "â­•ï¸" :"Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡",
    "ğŸ·" : "Ø³Ø§ÙƒØ³ÙÙˆÙ†",
    "ğŸ‘‹": "ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯",
    "âœŒğŸ¼": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸŒ":"Ù…Ø¨ØªØ³Ù…",
    "â¿"  : "Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡",
    "ğŸ’ªğŸ¼" : "Ù‚ÙˆÙŠ",
    "ğŸ“©":  "ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ",
    "â˜•ï¸": "Ù‚Ù‡ÙˆÙ‡",
    "ğŸ˜§" : "Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©",
    "ğŸ—¨": "Ø±Ø³Ø§Ù„Ø©",
    "â—ï¸" :"ØªØ¹Ø¬Ø¨",
    "ğŸ™†ğŸ»": "Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡",
    "ğŸ‘¯" :"Ø§Ø®ÙˆØ§Øª",
    "Â©" :  "Ø±Ù…Ø²",
    "ğŸ‘µğŸ½" :"Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡",
    "ğŸ£": "ÙƒØªÙƒÙˆØª",
    "ğŸ™Œ": "ØªØ´Ø¬ÙŠØ¹",
    "ğŸ™‡": "Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ",
    "ğŸ‘ğŸ½":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",
    "ğŸ‘ŒğŸ½": "Ø¨Ø§Ù„Ø¸Ø¨Ø·",
    "â‰ï¸" : "Ø§Ø³ØªÙ†ÙƒØ§Ø±",
    "âš½ï¸": "ÙƒÙˆØ±Ù‡",
    "ğŸ•¶" :"Ø­Ø¨",
    "ğŸˆ" :"Ø¨Ø§Ù„ÙˆÙ†",
    "ğŸ€":    "ÙˆØ±Ø¯Ù‡",
    "ğŸ’µ":  "ÙÙ„ÙˆØ³",
    "ğŸ˜‹":  "Ø¬Ø§Ø¦Ø¹",
    "ğŸ˜›":  "ÙŠØºÙŠØ¸",
    "ğŸ˜ ":  "ØºØ§Ø¶Ø¨",
    "âœğŸ»":  "ÙŠÙƒØªØ¨",
    "ğŸŒ¾":  "Ø§Ø±Ø²",
    "ğŸ‘£":  "Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†",
    "âŒ":"Ø±ÙØ¶",
    "ğŸŸ":"Ø·Ø¹Ø§Ù…",
    "ğŸ‘¬":"ØµØ¯Ø§Ù‚Ø©",
    "ğŸ°":"Ø§Ø±Ù†Ø¨",
    "â˜‚":"Ù…Ø·Ø±",
     "âšœ":"Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§",
    "ğŸ‘":"Ø®Ø±ÙˆÙ",
    "ğŸ—£":"ØµÙˆØª Ù…Ø±ØªÙØ¹",
    "ğŸ‘ŒğŸ¼":"Ø§Ø­Ø³Ù†Øª",
    "â˜˜":"Ù…Ø±Ø­",
    "ğŸ˜®":"ØµØ¯Ù…Ø©",
    "ğŸ˜¦":"Ù‚Ù„Ù‚",
    "â­•":"Ø§Ù„Ø­Ù‚",
    "âœï¸":"Ù‚Ù„Ù…",
    "â„¹":"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª",
    "ğŸ™ğŸ»":"Ø±ÙØ¶",
    "âšªï¸":"Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡",
    "ğŸ¤":"Ø­Ø²Ù†",
    "ğŸ’«":"Ù…Ø±Ø­",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ”":"Ø·Ø¹Ø§Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "âœˆï¸":"Ø³ÙØ±",
    "ğŸƒğŸ»â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ³":"Ø°ÙƒØ±",
    "ğŸ¤":"Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡",
    "ğŸ¾":"ÙƒØ±Ù‡",
    "ğŸ”":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ™‹":"Ø³Ø¤Ø§Ù„",
    "ğŸ“®":"Ø¨Ø­Ø±",
    "ğŸ’‰":"Ø¯ÙˆØ§Ø¡",
    "ğŸ™ğŸ¼":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’‚ğŸ¿ ":"Ø­Ø§Ø±Ø³",
    "ğŸ¬":"Ø³ÙŠÙ†Ù…Ø§",
    "â™¦ï¸":"Ù…Ø±Ø­",
    "ğŸ’¡":"Ù‚ÙƒØ±Ø©",
    "â€¼":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘¼":"Ø·ÙÙ„",
    "ğŸ”‘":"Ù…ÙØªØ§Ø­",
    "â™¥ï¸":"Ø­Ø¨",
    "ğŸ•‹":"ÙƒØ¹Ø¨Ø©",
    "ğŸ“":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ’©":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸ‘½":"ÙØ¶Ø§Ø¦ÙŠ",
    "â˜”ï¸":"Ù…Ø·Ø±",
    "ğŸ·":"Ø¹ØµÙŠØ±",
    "ğŸŒŸ":"Ù†Ø¬Ù…Ø©",
    "â˜ï¸":"Ø³Ø­Ø¨",
    "ğŸ‘ƒ":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸŒº":"Ù…Ø±Ø­",
     "ğŸ”ª":"Ø³ÙƒÙŠÙ†Ø©",
    "â™¨":"Ø³Ø®ÙˆÙ†ÙŠØ©",
    "ğŸ‘ŠğŸ¼":"Ø¶Ø±Ø¨",
    "âœ":"Ù‚Ù„Ù…",
    "ğŸš¶ğŸ¾â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "â—¾ï¸":"ÙˆÙ‚Ù",
    "ğŸ˜š":"Ø­Ø¨",
    "ğŸ”¸":"Ù…Ø±Ø­",
    "ğŸ‘ğŸ»":"Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ",
    "ğŸ‘ŠğŸ½":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜™":"Ø­Ø¨",
    "ğŸ¥":"ØªØµÙˆÙŠØ±",
    "ğŸ‘‰":"Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡",
    "ğŸ‘ğŸ½":"ÙŠØµÙÙ‚",
    "ğŸ’ªğŸ»":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ´":"Ø§Ø³ÙˆØ¯",
    "ğŸ”¥":"Ø­Ø±ÙŠÙ‚",
    "ğŸ˜¬":"Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©",
    "ğŸ‘ŠğŸ¿":"ÙŠØ¶Ø±Ø¨",
    "ğŸŒ¿":"ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡",
     "âœ‹ğŸ¼":"ÙƒÙ Ø§ÙŠØ¯",
    "ğŸ‘":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",
    "â˜ ï¸":"ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨",
    "ğŸ‰":"ÙŠÙ‡Ù†Ø¦",
    "ğŸ”•" :"ØµØ§Ù…Øª",
    "ğŸ˜¿":"ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†",
    "â˜¹ï¸":"ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³",
    "ğŸ˜˜" :"Ø­Ø¨",
    "ğŸ˜°" :"Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†",
    "ğŸŒ¼":"ÙˆØ±Ø¯Ù‡",
    "ğŸ’‹":  "Ø¨ÙˆØ³Ù‡",
    "ğŸ‘‡":"Ù„Ø§Ø³ÙÙ„",
    "â£ï¸":"Ø­Ø¨",
    "ğŸ§":"Ø³Ù…Ø§Ø¹Ø§Øª",
    "ğŸ“":"ÙŠÙƒØªØ¨",
    "ğŸ˜‡":"Ø¯Ø§ÙŠØ®",
    "ğŸ˜ˆ":"Ø±Ø¹Ø¨",
    "ğŸƒ":"ÙŠØ¬Ø±ÙŠ",
    "âœŒğŸ»":"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸ”«":"ÙŠØ¶Ø±Ø¨",
    "â—ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘":"ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚",
    "ğŸ”":"Ù‚ÙÙ„",
    "ğŸ‘ˆ":"Ù„Ù„ÙŠÙ…ÙŠÙ†",
    "â„¢":"Ø±Ù…Ø²",
    "ğŸš¶ğŸ½":"ÙŠØªÙ…Ø´ÙŠ",
    "ğŸ˜¯":"Ù…ØªÙØ§Ø¬Ø£",
    "âœŠ":"ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡",
    "ğŸ˜»":"Ø§Ø¹Ø¬Ø§Ø¨",
    "ğŸ™‰" :"Ù‚Ø±Ø¯",
    "ğŸ‘§":"Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡",
    "ğŸ”´":"Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡",
    "ğŸ’ªğŸ½":"Ù‚ÙˆÙ‡",
    "ğŸ’¤":"ÙŠÙ†Ø§Ù…",
    "ğŸ‘€":"ÙŠÙ†Ø¸Ø±",
    "âœğŸ»":"ÙŠÙƒØªØ¨",
    "â„ï¸":"ØªÙ„Ø¬",
    "ğŸ’€":"Ø±Ø¹Ø¨",
    "ğŸ˜¤":"ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³",
    "ğŸ–‹":"Ù‚Ù„Ù…",
    "ğŸ©":"ÙƒØ§Ø¨",
     "â˜•ï¸":"Ù‚Ù‡ÙˆÙ‡",
    "ğŸ˜¹":"Ø¶Ø­Ùƒ",
    "ğŸ’“":"Ø­Ø¨",
    "â˜„ï¸ ":"Ù†Ø§Ø±",
    "ğŸ‘»":"Ø±Ø¹Ø¨",
    }

emoticons_to_emoji = {
    ":)" : "ğŸ™‚",
    ":(" : "ğŸ™",
    "xD" : "ğŸ˜†",
    ":=(": "ğŸ˜­",
    ":'(": "ğŸ˜¢",
    ":'â€‘(": "ğŸ˜¢",
    "XD" : "ğŸ˜‚",
    ":D" : "ğŸ™‚",
    "â™¬" : "Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "â™¡" : "â¤",
    "â˜»"  : "ğŸ™‚",
    }

"""**Cleaning**"""

def remove_stop_words(text):
    Text=[i for i in str(text).split() if i not in arabic_stopwords]
    return " ".join(Text)

!pip install nltk

def ISRI_Stemmer(text):
    #making an object
    stemmer = ISRIStemmer()

    #stemming each word
    text = stemmer.stem(text)
    text = stemmer.pre32(text)
    text = stemmer.suf32(text)

    return text

def Snowball_stemmer(text):
    text = text.split()
    #making an object
    stemmer = SnowballStemmer("arabic")

    #stemming each word
    text=[stemmer.stem(y) for y in text]

    return " " .join(text)

def Arabic_Light_Stemmer(text):
    #making an object
    Arabic_Stemmer = ArabicLightStemmer()

    #stemming each word
    text=[Arabic_Stemmer.light_stem(y) for y in text.split()]

    return " " .join(text)

text = "Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙƒØ§Ù†Øª Ø¬Ù…ÙŠÙ„Ø©"
result = ISRI_Stemmer(text)
result

text = "Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙƒØ§Ù†Øª Ø¬Ù…ÙŠÙ„Ø©"
result = Snowball_stemmer(text)
result

text = "Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙƒØ§Ù†Øª Ø¬Ù…ÙŠÙ„Ø©"
result = Arabic_Light_Stemmer(text)
result

def normalizeArabic(text):
    text = text.strip()
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ø¡", text)
    text = re.sub("Ø¦", "Ø¡", text)
    text = re.sub("Ù‡","Ø©",text)

    text = re.sub("[Ø¥Ø£Ù±Ø¢Ø§]", "Ø§", text)
    text = text.replace('ÙˆÙˆ', 'Ùˆ')
    text = text.replace('ÙŠÙŠ', 'ÙŠ')
    text = text.replace('ÙŠÙŠÙŠ', 'ÙŠ')
    text = text.replace('Ø§Ø§', 'Ø§')

    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)

    # Remove longation
    text = re.sub(r'(.)\1+', r"\1\1", text)

    #Strip vowels from a text, include Shadda.
    text = araby.strip_tashkeel(text)

    #Strip diacritics from a text, include harakats and small lettres The striped marks are
    text = araby.strip_diacritics(text)
    text=''.join([i for i in text if not i.isdigit()])
    return text

def Removing_non_arabic(text):
    text = re.sub('[A-Za-z]+',' ',text)
    return text

def Removing_numbers(text):
    text=''.join([i for i in text if not i.isdigit()])
    return text

def Removing_punctuations(text):
    ## Remove punctuations
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\]^_`{|}~"""), ' ', text)
    text = text.replace('Ø›',"", )

    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    text =  " ".join(text.split())
    return text.strip()

def Removing_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def remove_emoji(string):

    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string).strip()

def remove_emoticons(text):
    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')
    return emoticon_pattern.sub(r'', text).strip()



def remove_extra_Space(text):
    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    return  " ".join(text.split())

def remove_small_sentences(df):
    for i in range(len(df)):
        if len(df.review_description.iloc[i].split()) < 3:
            df.review_description.iloc[i] = np.nan

def replace_emoticon_with_emojis(text):
    for i in range (len(text)):
        message = text.iloc[i]
        seperarate_word = message.split(' ')
        cleaned_df.review_description.iloc[i] = ""

        for word in seperarate_word:
            text.iloc[i] += emoticons_to_emoji.get(word, word) + " "

def replace_emojis_with_text(text):
    for i in range (len(text)):
            message = text.iloc[i]
            seperarate_word = regex.findall(r'\X', message)
            text.iloc[i] = ""

            for word in seperarate_word:
                if any(emoji.distinct_emoji_list(char)  for char in word):
                        text.iloc[i] += " " + emojis.get(word, word) + " "
                else:
                        text.iloc[i] +=  emojis.get(word, word) + ""

def space_between_emojis(s):
    return ''.join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in s)

def remove_hashtages_and_mentions(text):
    text = re.sub("@[A-Za-z0-9_]+","", text)
    text = re.sub("#[A-Za-z0-9_]+","", text)
    return text

cleaned_df = df

df.iloc[0].review_description

cleaned_df.iloc[0].review_description

replace_emoticon_with_emojis(cleaned_df.review_description)

# make a new column for emojis only

cleaned_df['only_emojis'] = cleaned_df['review_description'].apply(lambda x: extract_emoji(str(x)))
cleaned_df['only_emojis'] = cleaned_df['only_emojis'].apply(lambda x: ' '.join(x))

# remove emojis for column review_description
def remove_emojis_from_text(text):
    data = regex.findall(r'\X', text)
    return ''.join([char for char in data if not any(emoji.distinct_emoji_list(c) for c in char)])

cleaned_df['review_description'] = cleaned_df['review_description'].apply(lambda x: remove_emojis_from_text(str(x)))

cleaned_df

cleaned_df.text=cleaned_df.review_description.apply(lambda text : remove_stop_words(text))

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : Removing_non_arabic(text))

df.review_description.iloc[1]

cleaned_df.iloc[1].review_description

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : normalizeArabic(text))

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : Removing_numbers(text))

cleaned_df.iloc[1].review_description

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : remove_hashtages_and_mentions(text))

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : Removing_punctuations(text))

cleaned_df

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : ISRI_Stemmer(text))

cleaned_df.duplicated().sum()

cleaned_df.duplicated().sum()

cleaned_df=cleaned_df.drop_duplicates()

cleaned_df.duplicated().sum()

cleaned_df.head()

cleaned_df.isnull().sum()

import pandas as pd

# Ù…Ø«Ø§Ù„: Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ DataFrame Ø§Ø³Ù…Ù‡ cleaned_df
cleaned_df.to_csv("/content/cleaned_df.csv", index=False,encoding='utf-8-sig')

from IPython.display import FileLink

FileLink("/content/cleaned_df.csv")

!pip install python-bidi arabic-reshaper

"""# K_mean"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Step 1: Import Required Libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# Step 2: Generate Synthetic Data
X, y_true = make_blobs(n_samples=600, centers=3, cluster_std=0.70, random_state=0)

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Load and clean data
df = pd.read_csv('/content/cleaned_df (2).csv')
df = df.dropna(subset=['review_description'])

# Vectorize text
vectorizer = TfidfVectorizer(max_features=100)
X = vectorizer.fit_transform(df['review_description']).toarray()

# K-Means from scratch functions
def initialize_centroids(X, k):
    indices = np.random.choice(X.shape[0], size=k, replace=False)
    return X[indices]

def compute_distances(X, centroids):
    return np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)

def assign_clusters(distances):
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def k_means(X, k, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(X, k)
    for _ in range(max_iters):
        distances = compute_distances(X, centroids)
        labels = assign_clusters(distances)
        new_centroids = update_centroids(X, labels, k)
        if np.linalg.norm(new_centroids - centroids) < tol:
            break
        centroids = new_centroids
    return centroids, labels

# Run K-Means
centroids, labels = k_means(X, k=3)

# Optional: Add cluster labels to your DataFrame
df['cluster'] = labels

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù…Ù† 100 Ø¥Ù„Ù‰ 2
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Ø±Ø³Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹Ø§Øª (clusters)
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=10)
plt.title("K-Means Clusters (PCA Projection)")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.colorbar(scatter, label='Cluster')
plt.grid(True)
plt.show()

from sklearn.metrics import silhouette_score

# Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Silhouette Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
sil_score = silhouette_score(X, labels)
print(f"Silhouette Score: {sil_score:.2f}")

# Ø±Ø³Ù… Ù…Ø®Ø·Ø· Silhouette Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„ØªØ¬Ù…ÙŠØ¹
from sklearn.metrics import silhouette_samples
import numpy as np

fig, ax = plt.subplots(figsize=(10, 6))
ax.set_xlim([-1, 1])
ax.set_title('Silhouette Plot')
ax.set_xlabel('Silhouette Coefficient')
ax.set_ylabel('Cluster')

sample_silhouette_values = silhouette_samples(X, labels)

y_lower = 10
for i in range(3):  # Ø¨Ù…Ø§ Ø£Ù† Ù„Ø¯ÙŠÙ†Ø§ 3 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©
    ith_cluster_silhouette_values = sample_silhouette_values[labels == i]
    ith_cluster_silhouette_values.sort()

    y_upper = y_lower + len(ith_cluster_silhouette_values)
    ax.fill_betweenx(np.arange(y_lower, y_upper),
                     0, ith_cluster_silhouette_values, alpha=0.7, label=f"Cluster {i}")
    y_lower = y_upper + 10  # Ø¶Ø¹ ÙØ±Ø§Øº Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª

ax.legend()
plt.show()

"""#logistic regrision#"""

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

import pandas as pd

cleaned_df =pd.read_csv('/content/cleaned_df (2).csv')

cleaned_df .head()

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, weights):
    m = X.shape[0]
    h = sigmoid(np.dot(X, weights))
    epsilon = 1e-5
    cost = -(1/m) * (np.dot(y, np.log(h + epsilon)) + np.dot((1 - y), np.log(1 - h + epsilon)))
    return cost

def train_logistic_regression_with_metrics(X, y, lr=0.1, epochs=1000, X_val=None, y_val=None):
    m, n = X.shape
    weights = np.zeros(n)
    cost_history = []
    accuracy_history = []

    for i in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / m
        weights -= lr * gradient

        if i % 10 == 0:
            cost = compute_cost(X, y, weights)
            cost_history.append(cost)

            if X_val is not None:
                y_pred = predict(X_val, weights)
                acc = np.mean(y_pred == y_val)
                accuracy_history.append(acc)

    return weights, cost_history, accuracy_history

def predict(X, weights):
    probs = sigmoid(np.dot(X, weights))
    return [1 if p >= 0.5 else 0 for p in probs]

# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø£Ùˆ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ù†Øµ ÙØ§Ø±Øº
cleaned_df['review_description'] = cleaned_df['review_description'].fillna('')

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ bag of words
vectorizer = CountVectorizer(max_features=5000)
X = vectorizer.fit_transform(cleaned_df['review_description']).toarray()

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(cleaned_df['label'])



# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

weights, cost_history, accuracy_history = train_logistic_regression_with_metrics(
    X_train, y_train, lr=0.1, epochs=1000, X_val=X_test, y_val=y_test)

# Ø§Ù„ØªÙ†Ø¨Ø¤
y_pred = predict(X_test, weights)

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©
accuracy = np.mean(y_pred == y_test)
print(f"Logistic Regression Accuracy: {accuracy:.2f}")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer



plt.figure(figsize=(6,4))
plt.plot(range(0, 1000, 10), cost_history, color='darkorange')
plt.title('Logistic Regression Learning Curve (Cost over Epochs)')
plt.xlabel('Epoch')
plt.ylabel('Cost')
plt.grid(True)
plt.show()

plt.figure(figsize=(6,4))
plt.plot(range(0, 1000, 10), accuracy_history, color='green')
plt.title('Validation Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

softmax_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)

softmax_model.fit(X_train, y_train)

y_pred = softmax_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))







"""#without sklearn.#"""

import numpy as np

# Ø¯Ø§Ù„Ø© Softmax
def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø¯Ø¯ÙŠ
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ§Øª
X = vectorizer.fit_transform(cleaned_df['review_description']).toarray()
y = pd.get_dummies(cleaned_df['label']).values  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙØ¦Ø§Øª Ø¥Ù„Ù‰ one-hot encoding

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
n_features = X.shape[1]
n_classes = y.shape[1]
weights = np.random.randn(n_features, n_classes) * 0.01
bias = np.zeros(n_classes)
learning_rate = 0.01
n_epochs = 1000

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
for epoch in range(n_epochs):
    # Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
    z = np.dot(X, weights) + bias
    y_pred = softmax(z)

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© (Cross-Entropy)
    loss = -np.mean(np.sum(y * np.log(y_pred + 1e-10), axis=1))

    # Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ© (Gradient Descent)
    grad = y_pred - y
    grad_weights = np.dot(X.T, grad) / X.shape[0]
    grad_bias = np.mean(grad, axis=0)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
    weights -= learning_rate * grad_weights
    bias -= learning_rate * grad_bias

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Ø§Ù„ØªÙ†Ø¨Ø¤
z = np.dot(X, weights) + bias
y_pred = np.argmax(softmax(z), axis=1)
y_true = np.argmax(y, axis=1)
accuracy = np.mean(y_pred == y_true)
print("Accuracy:", accuracy)

"""Neural Network (FFNN)

in another notebook
"""