# -*- coding: utf-8 -*-
"""notebook37aaf711ea.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lmM9vCu3AT8urofApJ6Z77V2gDx-Cp7n
"""

!pip install Arabic-Stopwords

!pip install emoji

!pip install PyArabic

!pip install openpyxl

!pip install Tashaphyne

!pip install nltk.download('stopwords')

import pandas as pd

df=pd.read_csv('/content/Final_Data.csv')

df.head()

df.rename(columns ={'rating':'label'},inplace =True)

df.shape

df.info()

df.label.value_counts()

# Commented out IPython magic to ensure Python compatibility.
# Add environment Packages paths to conda
import os, sys, warnings
import pandas as pd
import numpy as np
warnings.simplefilter("ignore")

# Text preprocessing packages
import nltk # Text libarary
# nltk.download('stopwords')
import string # Removing special characters {#, @, ...}
import re # Regex Package
import regex
import emoji
# Corpora is a group presenting multiple collections of text documents. A single collection is called corpus.
from nltk.corpus import stopwords # Stopwords
import arabicstopwords.arabicstopwords as stp #more range of arabic stop words
from nltk.stem.isri import ISRIStemmer
import pyarabic.araby as araby
from tashaphyne.stemming import ArabicLightStemmer

from nltk.stem import SnowballStemmer, WordNetLemmatizer # Stemmer & Lemmatizer
#from gensim.utils import simple_preprocess  # Text ==> List of Tokens

# Text Embedding
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Visualization Packages
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(font_scale=1.3)
# %matplotlib inline

plt.figure(figsize=(8,4))
sns.countplot(x='label',data=df)

df.isnull().sum()

df[df['review_description'].isnull() == True]

df.duplicated().sum()

df[df['review_description'].duplicated() == True]

df=df.dropna()

df=df.drop_duplicates()

df.duplicated().sum()

df.isnull().sum()

import nltk
nltk.download('stopwords')

arabic_stopwords = stopwords.words("arabic")
arabic_stopwords

len(arabic_stopwords)

egyptian_stopwords_base = [
    'يعني', 'كده', 'أوي', 'بتاع', 'بتاعة', 'إزاي', 'إيه', 'بس', 'زي', 'دي',
    'ده', 'دا', 'فين', 'ليه', 'إمتى', 'مين',
    'كام', 'هنا', 'هناك', 'جوة', 'برا', 'فوق', 'تحت', 'جوه', 'قدام', 'ورا',
    'جنب', 'علشان', 'أهو', 'بقى', 'لسه', 'كدة', 'يعنيه', 'إحنا', 'إنت', 'إنتوا',
    'هو', 'هي', 'دول', 'كمان', 'برضو', 'أكيد', 'طب', 'بعدين', 'أيوه', 'لا'
    , 'كله', 'كلها', 'أي', 'بتوع', 'حاليًا', 'تو', 'دلوقتي', 'بكرة', 'إمبارح',
    'كل', 'كلنا', 'كلهم', 'بسرعة', 'شوية', 'شويتين', 'خالص', 'و', 'يا', 'لو',
    'لما', 'لمن', 'لغاية', 'منين', 'لحتى', 'علي', 'عن', 'قبل', 'بعد', 'تحتي',
    'فوقي', 'جنبي', 'ورايا', 'قدامي', 'جوايا', 'بره', 'أه', 'اها', 'إي', 'إوا',
    'بقا', 'بتقول', 'بتقولوا', 'يعنوا', 'كدي', 'إزيك', 'إزي', 'إزيكو', 'إزيه',
    'إزيهم', 'إزينا', 'إزيكم', 'طبعًا', 'أصلًا', 'فينك', 'فينكم', 'فينهم', 'فينه',
    'فينها', 'فينو', 'إيهده', 'إيدا', 'إيدي', 'إيدك', 'إيده', 'إيدها', 'إيدهو',
    'إيدهوم', 'كداك', 'كداه', 'كداها', 'كداهو', 'كداهم', 'انا', 'والله', 'شي',
    'كان', 'بعض', 'تم', 'فى'
]

egyptian_stopwords_extra = [
    'بتاعهم', 'بتاعنا', 'بتاعكم', 'بتاعي', 'بتاعه', 'بتاعها', 'بتاعو', 'زيك',
    'زيه', 'زيها', 'زيهم', 'زينا', 'زيكم', 'زيو', 'كديه', 'كدوه', 'كداها', 'كداهم',
    'كداهو', 'إزيكي', 'إزيكوا', 'إزيهم', 'إزينا', 'إزيكوم', 'فيني', 'فيك', 'فيهو',
    'فيها', 'فيهم', 'فيكم', 'فينا', 'إيدهي', 'إيدهوم', 'إيدينا', 'إيديك', 'إيديكم',
    'إيديهم', 'إيديه', 'بتقولي', 'بتقولو', 'بتقولهم', 'بتقولنا', 'بتقولك', 'بتقولكم',
    'يعنيكي', 'يعنيهم', 'يعنيني', 'يعنيكوم', 'يعنيها', 'يعنيهو', 'أهوه', 'أهي',
    'أهيا', 'أهيه', 'أهيهم', 'أهيها', 'أهيو', 'بقينا', 'بقيتوا', 'بقوا', 'بقيت',
    'بقيتي', 'بقي', 'لسنا', 'لسني', 'لسك',
    'لسكم', 'لسها', 'لسوا',  'أوكيكم', 'فينكي', 'فينكو', 'فينهم', 'فينهو',
    'فينها', 'فيني', 'فيكي', 'فيكو', 'إحناك', 'إحناه', 'إحناها', 'إحناهم', 'إحنانا',
    'إحناكم', 'إنتي', 'إنتو', 'إنتك', 'إنتها', 'إنتهم', 'إنتنا', 'إنتكم', 'هوي',
    'هوه', 'هوها', 'هوهم', 'هونا', 'هوكم', 'هيي', 'هيه', 'هيها', 'هيهم', 'هينا',
    'هيكم', 'دولي', 'دوله', 'دولها', 'دولهم', 'دولنا', 'دولكم', 'ديي', 'ديه',
    'ديها', 'ديهم', 'دينا', 'ديكم', 'كماني', 'كمانه', 'كمانها', 'كمانهم', 'كماننا',
    'كمانكم', 'برضوي', 'برضوه', 'برضوها', 'برضوهم', 'برضونا', 'برضوكم', 'أكيدي',
    'أكيده', 'أكيدها', 'أكيدهم', 'أكيدنا', 'أكيدكم', 'طبي', 'طبه', 'طبها', 'طبهم',
    'طبنا', 'طبكم', 'بعديني', 'بعدينه', 'بعدينها', 'بعدينهم', 'بعديننا', 'بعدينكم',
    'أيوهي', 'أيوهه', 'أيوهها', 'أيوههم', 'أيوهنا', 'أيوهكم', 'لاه', 'لاها', 'لاهم',
    'لانا', 'لاكم', 'عاديها', 'عاديهم', 'عادينا', 'عاديكم', 'كلهي',
    'كلهه', 'كلهها', 'كلههم', 'كلني', 'كلنه', 'كلنها', 'كلنهم', 'كلننا', 'كلنكم',
    'بسرعته', 'بسرعتها', 'بسرعتهم', 'بسرعتنا', 'بسرعتكم', 'شويه', 'شويات', 'شوينا',
    'شويك', 'شويكم', 'شويهم', 'شويها', 'شويهو', 'خالصي', 'خالصه', 'خالصها', 'خالصهم',
    'خالصنا', 'خالصكم', 'وو', 'ويا', 'ولو', 'ولما', 'ولمن', 'ولغاية', 'ومنين', 'ولحتى',
    'وعلي', 'وعن', 'وقبل', 'وبعد', 'وتحت', 'وفوق', 'وجنب', 'وورا', 'وقدام', 'وجوة',
    'وبره', 'وأه', 'واها', 'وإي', 'وإوا', 'وبقا', 'وبتقول', 'وبتقولوا', 'ويعني', 'وكده',
    'وإزاي', 'وإيه', 'وبس', 'وزي', 'ودي', 'وده', 'ودا', 'وماشي', 'وطيب', 'وتمام',
    'وأوكي', 'وفين', 'وليه', 'وإمتى', 'ومين', 'وكام', 'وهنا', 'وهناك', 'وجوة', 'وبرا',
    'وفوق', 'وتحت', 'وجوه', 'وقدام', 'وورا', 'وجنب', 'وعلشان', 'وأهو', 'وبقى', 'ولسه',
    'وكدة', 'ويعنيه', 'وإحنا', 'وإنت', 'وإنتوا', 'وهو', 'وهي', 'ودول', 'وكمان', 'وبرضو',
    'وأكيد', 'وطب', 'وبعدين', 'وأيوه', 'ولا', 'وعادي', 'وكله', 'وكلها', 'وأي', 'وبتوع',
    'وحاليًا', 'وتو', 'ودلوقتي', 'وبكرة', 'وإمبارح', 'وكل', 'وكلنا', 'وكلهم', 'وبسرعة',
    'إللي', 'اللي', 'الي', 'الل', 'ال', 'الو', 'الي', 'الليي', 'الليه', 'الليها', 'الليهم',
    'اللينا', 'الليكم', 'عليه', 'عليها', 'عليهم', 'علينا', 'عليكم', 'عنه', 'عنها', 'عنهم',
    'عننا', 'عنكم', 'قبله', 'قبلها', 'قبلهم', 'قبلنا', 'قبلكم', 'بعده', 'بعدها', 'بعدهم',
    'بعدنا', 'بعدكم', 'تحته', 'تحتها', 'تحتهم', 'تحتنا', 'تحتكم', 'فوقه', 'فوقها', 'فوقهم',
    'فوقنا', 'فوقكم', 'جنبه', 'جنبها', 'جنبهم', 'جنبنا', 'جنبكم', 'وراه', 'وراها', 'وراهم',
    'ورانا', 'وراكم', 'قدامه', 'قدامها', 'قدامهم', 'قدامنا', 'قدامكم', 'جواه', 'جواها',
    'جواهم', 'جوانا', 'جواكم', 'براه', 'براها', 'براهم', 'برانا', 'براكم', 'أهي', 'أهيه',
    'أهيها', 'أهيهم', 'أهينا', 'أهيكم', 'بقاه', 'بقاها', 'بقاهم', 'بقانا', 'بقاكم', 'لسهي',
    'لسهه', 'لسهها', 'لسههم', 'لسنا', 'لسهكم'
]

from collections import Counter

df['review_description'] = df['review_description'].astype(str)
all_text = ' '.join(df['review_description'])
words = all_text.split()
word_freq = Counter(words)

# طباعة أكثر 20 كلمة شيوعًا
print("the most 100 word frequent", word_freq.most_common(100))

df['stop_words'] = df['review_description'].apply(lambda x: len(set(x.split()) & set(arabic_stopwords)))
df.stop_words.value_counts()

df.head()

sns.histplot(df['stop_words'], bins=20, color='purple', kde=False)
plt.title('Distribution of Stop Words in Texts', fontsize=14)
plt.xlabel('Number of Stop Words', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.show()

def emoji_counter(sentence):
    return emoji.emoji_count(sentence)

df['emoji_count'] = df['review_description'].apply(lambda x: emoji_counter(x))
df.emoji_count.value_counts()[:10]

df.head()

emojis_data = df[df['emoji_count'] > 0]

df.info()

emojis_data

df = df.drop(df[df['review_description'] == "جحهنناغنمجظ جحختاىةزونه😋😋😋😋😋😋😋😋😋"].index)

emojis_data

def extract_emoji(text):
    emoji_list = []
    data = regex.findall(r'\X', text)
    for word in data:
        if any(emoji.distinct_emoji_list(char) for char in word):
            emoji_list.append(word)

    return emoji_list

emojis_data['emoji'] = emojis_data['review_description'].apply(lambda x: extract_emoji(x))
emojis_data['emoji'] = emojis_data['emoji'].apply(lambda x: ' '.join([word for word in x ]))

emojis_data

pd.Series(' '.join(emojis_data['emoji']).split()).value_counts()[:10]

emojis = {
    "🙂":"يبتسم",
    "😂":"يضحك",
    "💔":"قلب حزين",
    "🙂":"يبتسم",
    "❤️":"حب",
    "❤":"حب",
    "😍":"حب",
    "😭":"يبكي",
    "😢":"حزن",
    "😔":"حزن",
    "♥":"حب",
    "💜":"حب",
    "😅":"يضحك",
    "🙁":"حزين",
    "💕":"حب",
    "💙":"حب",
    "😞":"حزين",
    "😊":"سعادة",
    "👏":"يصفق",
    "👌":"احسنت",
    "😴":"ينام",
    "😀":"يضحك",
 "😌":"حزين",
    "🌹":"وردة",
    "🙈":"حب",
    "😄":"يضحك",
    "😐":"محايد",
    "✌":"منتصر",
    "✨":"نجمه",
    "🤔":"تفكير",
    "😏":"يستهزء",
    "😒":"يستهزء",
    "🙄":"ملل",
    "😕":"عصبية",
    "😃":"يضحك",
    "🌸":"وردة",
    "😓":"حزن",
    "💞":"حب",
    "💗":"حب",
    "😑":"منزعج",
    "💭":"تفكير",
    "😎":"ثقة",
    "💛":"حب",
    "😩":"حزين",
    "💪":"عضلات",
    "👍":"موافق",
    "🙏🏻":"رجاء طلب",
    "😳":"مصدوم",
    "👏🏼":"تصفيق",
    "🎶":"موسيقي",
    "🌚":"صمت",
    "💚":"حب",
    "🙏":"رجاء طلب",
    "💘":"حب",
    "🍃":"سلام",
    "☺":"يضحك",
    "🐸":"ضفدع",
    "😶":"مصدوم",
    "✌️":"مرح",
    "✋🏻":"توقف",
    "😉":"غمزة",
    "🌷":"حب",
    "🙃":"مبتسم",
    "😫":"حزين",
    "😨":"مصدوم",
    "🎼 ":"موسيقي",
    "🍁":"مرح",
    "🍂":"مرح",
    "💟":"حب",
    "😪":"حزن",
    "😆":"يضحك",
    "😣":"استياء",
    "☺️":"حب",
    "😱":"كارثة",
    "😁":"يضحك",
    "😖":"استياء",
    "🏃🏼":"يجري",
    "😡":"غضب",
    "🚶":"يسير",
    "🤕":"مرض",
    "‼️":"تعجب",
    "🕊":"طائر",
    "👌🏻":"احسنت",
    "❣":"حب",
    "🙊":"مصدوم",
    "💃":"سعادة مرح",
    "💃🏼":"سعادة مرح",
    "😜":"مرح",
    "👊":"ضربة",
    "😟":"استياء",
    "💖":"حب",
    "😥":"حزن",
    "🎻":"موسيقي",
    "✒":"يكتب",
    "🚶🏻":"يسير",
    "💎":"الماظ",
    "😷":"وباء مرض",
    "☝":"واحد",
    "🚬":"تدخين",
    "💐" : "ورد",
    "🌞" : "شمس",
    "👆" : "الاول",
    "⚠️" :"تحذير",
    "🤗" : "احتواء",
    "✖️": "غلط",
    "📍"  : "مكان",
    "👸" : "ملكه",
    "👑" : "تاج",
    "✔️" : "صح",
    "💌": "قلب",
     "😲" : "مندهش",
    "💦": "ماء",
    "🚫" : "خطا",
    "👏🏻" : "برافو",
    "🏊" :"يسبح",
    "👍🏻": "تمام",
    "⭕️" :"دائره كبيره",
    "🎷" : "ساكسفون",
    "👋": "تلويح باليد",
    "✌🏼": "علامه النصر",
    "🌝":"مبتسم",
    "➿"  : "عقده مزدوجه",
    "💪🏼" : "قوي",
    "📩":  "تواصل معي",
    "☕️": "قهوه",
    "😧" : "قلق و صدمة",
    "🗨": "رسالة",
    "❗️" :"تعجب",
    "🙆🏻": "اشاره موافقه",
    "👯" :"اخوات",
    "©" :  "رمز",
    "👵🏽" :"سيده عجوزه",
    "🐣": "كتكوت",
    "🙌": "تشجيع",
    "🙇": "شخص ينحني",
    "👐🏽":"ايدي مفتوحه",
    "👌🏽": "بالظبط",
    "⁉️" : "استنكار",
    "⚽️": "كوره",
    "🕶" :"حب",
    "🎈" :"بالون",
    "🎀":    "ورده",
    "💵":  "فلوس",
    "😋":  "جائع",
    "😛":  "يغيظ",
    "😠":  "غاضب",
    "✍🏻":  "يكتب",
    "🌾":  "ارز",
    "👣":  "اثر قدمين",
    "❌":"رفض",
    "🍟":"طعام",
    "👬":"صداقة",
    "🐰":"ارنب",
    "☂":"مطر",
     "⚜":"مملكة فرنسا",
    "🐑":"خروف",
    "🗣":"صوت مرتفع",
    "👌🏼":"احسنت",
    "☘":"مرح",
    "😮":"صدمة",
    "😦":"قلق",
    "⭕":"الحق",
    "✏️":"قلم",
    "ℹ":"معلومات",
    "🙍🏻":"رفض",
    "⚪️":"نضارة نقاء",
    "🐤":"حزن",
    "💫":"مرح",
    "💝":"حب",
    "🍔":"طعام",
    "❤︎":"حب",
    "✈️":"سفر",
    "🏃🏻‍♀️":"يسير",
    "🍳":"ذكر",
    "🎤":"مايك غناء",
    "🎾":"كره",
    "🐔":"دجاجة",
    "🙋":"سؤال",
    "📮":"بحر",
    "💉":"دواء",
    "🙏🏼":"رجاء طلب",
    "💂🏿 ":"حارس",
    "🎬":"سينما",
    "♦️":"مرح",
    "💡":"قكرة",
    "‼":"تعجب",
    "👼":"طفل",
    "🔑":"مفتاح",
    "♥️":"حب",
    "🕋":"كعبة",
    "🐓":"دجاجة",
    "💩":"معترض",
    "👽":"فضائي",
    "☔️":"مطر",
    "🍷":"عصير",
    "🌟":"نجمة",
    "☁️":"سحب",
    "👃":"معترض",
    "🌺":"مرح",
     "🔪":"سكينة",
    "♨":"سخونية",
    "👊🏼":"ضرب",
    "✏":"قلم",
    "🚶🏾‍♀️":"يسير",
    "👊":"ضربة",
    "◾️":"وقف",
    "😚":"حب",
    "🔸":"مرح",
    "👎🏻":"لا يعجبني",
    "👊🏽":"ضربة",
    "😙":"حب",
    "🎥":"تصوير",
    "👉":"جذب انتباه",
    "👏🏽":"يصفق",
    "💪🏻":"عضلات",
    "🏴":"اسود",
    "🔥":"حريق",
    "😬":"عدم الراحة",
    "👊🏿":"يضرب",
    "🌿":"ورقه شجره",
     "✋🏼":"كف ايد",
    "👐":"ايدي مفتوحه",
    "☠️":"وجه مرعب",
    "🎉":"يهنئ",
    "🔕" :"صامت",
    "😿":"وجه حزين",
    "☹️":"وجه يائس",
    "😘" :"حب",
    "😰" :"خوف و حزن",
    "🌼":"ورده",
    "💋":  "بوسه",
    "👇":"لاسفل",
    "❣️":"حب",
    "🎧":"سماعات",
    "📝":"يكتب",
    "😇":"دايخ",
    "😈":"رعب",
    "🏃":"يجري",
    "✌🏻":"علامه النصر",
    "🔫":"يضرب",
    "❗️":"تعجب",
    "👎":"غير موافق",
    "🔐":"قفل",
    "👈":"لليمين",
    "™":"رمز",
    "🚶🏽":"يتمشي",
    "😯":"متفاجأ",
    "✊":"يد مغلقه",
    "😻":"اعجاب",
    "🙉" :"قرد",
    "👧":"طفله صغيره",
    "🔴":"دائره حمراء",
    "💪🏽":"قوه",
    "💤":"ينام",
    "👀":"ينظر",
    "✍🏻":"يكتب",
    "❄️":"تلج",
    "💀":"رعب",
    "😤":"وجه عابس",
    "🖋":"قلم",
    "🎩":"كاب",
     "☕️":"قهوه",
    "😹":"ضحك",
    "💓":"حب",
    "☄️ ":"نار",
    "👻":"رعب",
    }

emoticons_to_emoji = {
    ":)" : "🙂",
    ":(" : "🙁",
    "xD" : "😆",
    ":=(": "😭",
    ":'(": "😢",
    ":'‑(": "😢",
    "XD" : "😂",
    ":D" : "🙂",
    "♬" : "موسيقي",
    "♡" : "❤",
    "☻"  : "🙂",
    }

"""**Cleaning**"""

def remove_stop_words(text):
    Text=[i for i in str(text).split() if i not in arabic_stopwords]
    return " ".join(Text)

!pip install nltk

def ISRI_Stemmer(text):
    #making an object
    stemmer = ISRIStemmer()

    #stemming each word
    text = stemmer.stem(text)
    text = stemmer.pre32(text)
    text = stemmer.suf32(text)

    return text

def Snowball_stemmer(text):
    text = text.split()
    #making an object
    stemmer = SnowballStemmer("arabic")

    #stemming each word
    text=[stemmer.stem(y) for y in text]

    return " " .join(text)

def Arabic_Light_Stemmer(text):
    #making an object
    Arabic_Stemmer = ArabicLightStemmer()

    #stemming each word
    text=[Arabic_Stemmer.light_stem(y) for y in text.split()]

    return " " .join(text)

text = "المكتبة كانت جميلة"
result = ISRI_Stemmer(text)
result

text = "المكتبة كانت جميلة"
result = Snowball_stemmer(text)
result

text = "المكتبة كانت جميلة"
result = Arabic_Light_Stemmer(text)
result

def normalizeArabic(text):
    text = text.strip()
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "ء", text)
    text = re.sub("ئ", "ء", text)
    text = re.sub("ه","ة",text)

    text = re.sub("[إأٱآا]", "ا", text)
    text = text.replace('وو', 'و')
    text = text.replace('يي', 'ي')
    text = text.replace('ييي', 'ي')
    text = text.replace('اا', 'ا')

    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)

    # Remove longation
    text = re.sub(r'(.)\1+', r"\1\1", text)

    #Strip vowels from a text, include Shadda.
    text = araby.strip_tashkeel(text)

    #Strip diacritics from a text, include harakats and small lettres The striped marks are
    text = araby.strip_diacritics(text)
    text=''.join([i for i in text if not i.isdigit()])
    return text

def Removing_non_arabic(text):
    text = re.sub('[A-Za-z]+',' ',text)
    return text

def Removing_numbers(text):
    text=''.join([i for i in text if not i.isdigit()])
    return text

def Removing_punctuations(text):
    ## Remove punctuations
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', text)
    text = text.replace('؛',"", )

    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    text =  " ".join(text.split())
    return text.strip()

def Removing_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def remove_emoji(string):

    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string).strip()

def remove_emoticons(text):
    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')
    return emoticon_pattern.sub(r'', text).strip()



def remove_extra_Space(text):
    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    return  " ".join(text.split())

def remove_small_sentences(df):
    for i in range(len(df)):
        if len(df.review_description.iloc[i].split()) < 3:
            df.review_description.iloc[i] = np.nan

def replace_emoticon_with_emojis(text):
    for i in range (len(text)):
        message = text.iloc[i]
        seperarate_word = message.split(' ')
        cleaned_df.review_description.iloc[i] = ""

        for word in seperarate_word:
            text.iloc[i] += emoticons_to_emoji.get(word, word) + " "

def replace_emojis_with_text(text):
    for i in range (len(text)):
            message = text.iloc[i]
            seperarate_word = regex.findall(r'\X', message)
            text.iloc[i] = ""

            for word in seperarate_word:
                if any(emoji.distinct_emoji_list(char)  for char in word):
                        text.iloc[i] += " " + emojis.get(word, word) + " "
                else:
                        text.iloc[i] +=  emojis.get(word, word) + ""

def space_between_emojis(s):
    return ''.join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in s)

def remove_hashtages_and_mentions(text):
    text = re.sub("@[A-Za-z0-9_]+","", text)
    text = re.sub("#[A-Za-z0-9_]+","", text)
    return text

cleaned_df = df

df.iloc[0].review_description

cleaned_df.iloc[0].review_description

replace_emoticon_with_emojis(cleaned_df.review_description)

# make a new column for emojis only

cleaned_df['only_emojis'] = cleaned_df['review_description'].apply(lambda x: extract_emoji(str(x)))
cleaned_df['only_emojis'] = cleaned_df['only_emojis'].apply(lambda x: ' '.join(x))

# remove emojis for column review_description
def remove_emojis_from_text(text):
    data = regex.findall(r'\X', text)
    return ''.join([char for char in data if not any(emoji.distinct_emoji_list(c) for c in char)])

cleaned_df['review_description'] = cleaned_df['review_description'].apply(lambda x: remove_emojis_from_text(str(x)))

cleaned_df

cleaned_df.text=cleaned_df.review_description.apply(lambda text : remove_stop_words(text))

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : Removing_non_arabic(text))

df.review_description.iloc[1]

cleaned_df.iloc[1].review_description

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : normalizeArabic(text))

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : Removing_numbers(text))

cleaned_df.iloc[1].review_description

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : remove_hashtages_and_mentions(text))

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : Removing_punctuations(text))

cleaned_df

cleaned_df.review_description=cleaned_df.review_description.apply(lambda text : ISRI_Stemmer(text))

cleaned_df.duplicated().sum()

cleaned_df.duplicated().sum()

cleaned_df=cleaned_df.drop_duplicates()

cleaned_df.duplicated().sum()

cleaned_df.head()

cleaned_df.isnull().sum()

import pandas as pd

# مثال: لو عندك DataFrame اسمه cleaned_df
cleaned_df.to_csv("/content/cleaned_df.csv", index=False,encoding='utf-8-sig')

from IPython.display import FileLink

FileLink("/content/cleaned_df.csv")

!pip install python-bidi arabic-reshaper

"""# K_mean"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Step 1: Import Required Libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# Step 2: Generate Synthetic Data
X, y_true = make_blobs(n_samples=600, centers=3, cluster_std=0.70, random_state=0)

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Load and clean data
df = pd.read_csv('/content/cleaned_df (2).csv')
df = df.dropna(subset=['review_description'])

# Vectorize text
vectorizer = TfidfVectorizer(max_features=100)
X = vectorizer.fit_transform(df['review_description']).toarray()

# K-Means from scratch functions
def initialize_centroids(X, k):
    indices = np.random.choice(X.shape[0], size=k, replace=False)
    return X[indices]

def compute_distances(X, centroids):
    return np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)

def assign_clusters(distances):
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def k_means(X, k, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(X, k)
    for _ in range(max_iters):
        distances = compute_distances(X, centroids)
        labels = assign_clusters(distances)
        new_centroids = update_centroids(X, labels, k)
        if np.linalg.norm(new_centroids - centroids) < tol:
            break
        centroids = new_centroids
    return centroids, labels

# Run K-Means
centroids, labels = k_means(X, k=3)

# Optional: Add cluster labels to your DataFrame
df['cluster'] = labels

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# تقليل الأبعاد من 100 إلى 2
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# رسم التجميعات (clusters)
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=10)
plt.title("K-Means Clusters (PCA Projection)")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.colorbar(scatter, label='Cluster')
plt.grid(True)
plt.show()

from sklearn.metrics import silhouette_score

# حساب درجة Silhouette للمجموعات
sil_score = silhouette_score(X, labels)
print(f"Silhouette Score: {sil_score:.2f}")

# رسم مخطط Silhouette لعرض الجودة البصرية للتجميع
from sklearn.metrics import silhouette_samples
import numpy as np

fig, ax = plt.subplots(figsize=(10, 6))
ax.set_xlim([-1, 1])
ax.set_title('Silhouette Plot')
ax.set_xlabel('Silhouette Coefficient')
ax.set_ylabel('Cluster')

sample_silhouette_values = silhouette_samples(X, labels)

y_lower = 10
for i in range(3):  # بما أن لدينا 3 مجموعات
    # تحديد النقاط الخاصة بكل مجموعة
    ith_cluster_silhouette_values = sample_silhouette_values[labels == i]
    ith_cluster_silhouette_values.sort()

    y_upper = y_lower + len(ith_cluster_silhouette_values)
    ax.fill_betweenx(np.arange(y_lower, y_upper),
                     0, ith_cluster_silhouette_values, alpha=0.7, label=f"Cluster {i}")
    y_lower = y_upper + 10  # ضع فراغ بين المجموعات

ax.legend()
plt.show()

"""#logistic regrision#"""

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

import pandas as pd

cleaned_df =pd.read_csv('/content/cleaned_df (2).csv')

cleaned_df .head()

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, weights):
    m = X.shape[0]
    h = sigmoid(np.dot(X, weights))
    epsilon = 1e-5
    cost = -(1/m) * (np.dot(y, np.log(h + epsilon)) + np.dot((1 - y), np.log(1 - h + epsilon)))
    return cost

def train_logistic_regression_with_metrics(X, y, lr=0.1, epochs=1000, X_val=None, y_val=None):
    m, n = X.shape
    weights = np.zeros(n)
    cost_history = []
    accuracy_history = []

    for i in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / m
        weights -= lr * gradient

        if i % 10 == 0:
            cost = compute_cost(X, y, weights)
            cost_history.append(cost)

            if X_val is not None:
                y_pred = predict(X_val, weights)
                acc = np.mean(y_pred == y_val)
                accuracy_history.append(acc)

    return weights, cost_history, accuracy_history

def predict(X, weights):
    probs = sigmoid(np.dot(X, weights))
    return [1 if p >= 0.5 else 0 for p in probs]

# إزالة القيم المفقودة أو استبدالها بنص فارغ
cleaned_df['review_description'] = cleaned_df['review_description'].fillna('')

# تحويل النصوص إلى bag of words
vectorizer = CountVectorizer(max_features=5000)
X = vectorizer.fit_transform(cleaned_df['review_description']).toarray()

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(cleaned_df['label'])



# تقسيم البيانات
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

weights, cost_history, accuracy_history = train_logistic_regression_with_metrics(
    X_train, y_train, lr=0.1, epochs=1000, X_val=X_test, y_val=y_test)

# التنبؤ
y_pred = predict(X_test, weights)

# حساب الدقة
accuracy = np.mean(y_pred == y_test)
print(f"Logistic Regression Accuracy: {accuracy:.2f}")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer



plt.figure(figsize=(6,4))
plt.plot(range(0, 1000, 10), cost_history, color='darkorange')
plt.title('Logistic Regression Learning Curve (Cost over Epochs)')
plt.xlabel('Epoch')
plt.ylabel('Cost')
plt.grid(True)
plt.show()

plt.figure(figsize=(6,4))
plt.plot(range(0, 1000, 10), accuracy_history, color='green')
plt.title('Validation Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

softmax_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)

softmax_model.fit(X_train, y_train)

y_pred = softmax_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))







"""#without sklearn.#"""

import numpy as np

# دالة Softmax
def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # للاستقرار العددي
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# تحويل البيانات إلى مصفوفات
X = vectorizer.fit_transform(cleaned_df['review_description']).toarray()
y = pd.get_dummies(cleaned_df['label']).values  # تحويل الفئات إلى one-hot encoding

# تهيئة المتغيرات
n_features = X.shape[1]
n_classes = y.shape[1]
weights = np.random.randn(n_features, n_classes) * 0.01
bias = np.zeros(n_classes)
learning_rate = 0.01
n_epochs = 1000

# تدريب النموذج
for epoch in range(n_epochs):
    # الحسابات الأمامية
    z = np.dot(X, weights) + bias
    y_pred = softmax(z)

    # حساب الخسارة (Cross-Entropy)
    loss = -np.mean(np.sum(y * np.log(y_pred + 1e-10), axis=1))

    # الحسابات العكسية (Gradient Descent)
    grad = y_pred - y
    grad_weights = np.dot(X.T, grad) / X.shape[0]
    grad_bias = np.mean(grad, axis=0)

    # تحديث الأوزان
    weights -= learning_rate * grad_weights
    bias -= learning_rate * grad_bias

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# التنبؤ
z = np.dot(X, weights) + bias
y_pred = np.argmax(softmax(z), axis=1)
y_true = np.argmax(y, axis=1)
accuracy = np.mean(y_pred == y_true)
print("Accuracy:", accuracy)

"""Neural Network (FFNN)

in another notebook
"""